stat4arch
========================================================
author: Petr Pajdla & Peter Tkáč
autosize: true
width: 1920
height: 1080

AES_707: *Statistics seminar for archaeologists*

<!-- <https://support.rstudio.com/hc/en-us/articles/200486468> -->

<!-- =============================================== -->

Seminar 6
========================================================
type: section

`28. 4. 2022`

Today:
<ul style='color:white;'>
<li>Multivariate statistics
<li>Dimensionality reduction
<li>Clustering
</ul>

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
```


<!-- =============================================== -->

Multivariate data & methods
========================================================
type: section

<!-- ----------------------------------------------- -->

Multivariate data
========================================================

### Division based on number of studied variables

- Univariate  
(one variable per observation);
- Bivariate  
(two variables);
- **Multivariate**  
(also *multidimensional*, multiple variables).

***

<img src="fig/mvn.png" style='box-shadow:none; width:100%;'>

<!-- ----------------------------------------------- -->

Dimensionality reduction
========================================================
type: sub-section

<!-- ----------------------------------------------- -->

The curse of higher dimensions
========================================================
title: false
left: 60%
incremental: true

<img src="fig/curse.png" style="width:100%; box-shadow:none;">

***

The *curse* of higher dimensions...

- Computational ineffectivity;
- Low data density in higher dimensions;
- Problematic visualization, we do not easily cope with more than 3D;
- Difficult interpretation *and much more*.


<!-- ----------------------------------------------- -->

Principal components analysis
========================================================

**Goal:** Find low-dimensional representation of the observations that explain a good fraction of the variation.

- First principal component is a direction that maximizes the 
variance of the projected data.
- Second PC is orthogonal to the previous one.

<img src='fig/pca.png' style="width:100%; box-shadow:none;">

<!-- ----------------------------------------------- -->

PCA I: Data
========================================================
left: 60%
incremental: true

### Data preparation
- Works with numeric data only.
- Format the input as a **matrix**...

```{r pca1}
data("DartPoints", package = "archdata")
```
```{r pca1-1}
dp <- DartPoints %>% 
  select(Length, Width) %>% # select columns
  as.matrix() # format as a matrix
```
```{r pca1-2}
# set row names with IDs
rownames(dp) <- DartPoints$Catalog
head(dp, 4)
```

***

### Plot the original data
```{r pca2}
plot(dp)
```

<!-- ----------------------------------------------- -->

PCA II
========================================================
left: 60%
incremental: true

### Do the PCA

```{r pca3}
dp_pca <- prcomp(dp)
dp_pca
summary(dp_pca)
```

***

```{r pca4}
plot(dp_pca)
```

*Scree* plot

- Shows the amount of variation explained by each PC.

<!-- ----------------------------------------------- -->

PCA III: Biplot
========================================================

```{r pca5, fig.width=9, fig.height=9}
biplot(dp_pca)
```

***

```{r pca6, fig.width=9, fig.height=9}
ggbiplot::ggbiplot(dp_pca)
```

<!-- ----------------------------------------------- -->

PCA IV: The details
========================================================
incremental: true

```{r pca7}
# structure of the prcomp object
str(dp_pca)
```

***

```{r pca8}
# extract the coordinates
dp_pca_x <- dp_pca$x
head(dp_pca_x)
class(dp_pca_x)
dp_pca_x <- as.data.frame(dp_pca_x)
class(dp_pca_x)
```

<!-- ----------------------------------------------- -->

PCA V: Custom plot
========================================================
left: 65%

```{r pca9, eval=FALSE, echo=FALSE}
# p1 <- dp_pca_x %>% 
#   bind_cols(Name = DartPoints$Name) %>% 
#   ggplot(aes(PC1, PC2, color = Name)) +
#   geom_point() +
#   coord_fixed() +
#   theme_minimal() +
#   labs(title = "PCA space of DartPoints Length and Width")
# 
# p2 <- DartPoints %>% 
#   ggplot(aes(Length, Width, color = Name)) +
#   geom_point() +
#   coord_fixed() +
#   theme_minimal() +
#   labs(title = "DartPoints scatterplot of Length and Width")
# 
# library(patchwork)
# p <- p1 / p2
# 
# ggsave(here::here("lectures/seminar6/fig/pca_dp.png"), p, 
#        width = 14, height = 8)
```

<img src="fig/pca_dp.png">

***

```{r eval=FALSE}
dp_pca_x %>% 
  bind_cols(
    type = DartPoints$Name
  ) %>% 
  ggplot(
    aes(PC1, PC2, 
        color = type)
  ) +
  geom_point()
```

<!-- =============================================== -->

Clustering
========================================================
type: sub-section

***

<img src="fig/clustering.jpeg" style="width:60%;">

<!-- ----------------------------------------------- -->

Clustering
========================================================
left: 76%
incremental: true

- Broad set of techniques for finding **subgroups**.
- Observations **within groups are quite similar to each other** and observations **in different groups quite different from each other**.

### Clustering methods

- Partitioning: **k-means clustering**
- Agglomerative/divisive: **herarchical clustering**
- Model-based: mixture models

***

<img src="fig/kmeans.jpeg">  
<img src="fig/hierarchical_clust.png">

<!-- ----------------------------------------------- -->

K-means partitioning
========================================================
type: sub-section

***

<img src="fig/kebab_kiosks.jpg">

<!-- ----------------------------------------------- -->

K-means partitioning
========================================================

- Simple approach to partitioning a data set into **K** distinct,
non-overlapping clusters.
- **K** is specified beforehand.

<img src="fig/kmeans_nr_K.png" style="width:76%;">  
<p style="font-size:60%;">(James et al. 2013)</p>

<!-- ----------------------------------------------- -->

K-means algorithm
========================================================
left: 42%
title: false
incremental: true

## K-means algorithm

- *K* ... number of clusters.  

### The process: 

1. **Randomly** assign each observation to groups 1 to *K*;
2. For each of *K* clusters, derive its **centroid** (midpoint);
3. **Reassign** each observation into a cluster whose centroid is 
the **closest**;
4. Iterate until stability in cluster assignments is reached (*local vs global optima*).

***

<img src="fig/kmeans_algorithm.png" style="width:80%;">  
<p style="font-size:60%;">(James et al. 2013)</p>

<!-- ----------------------------------------------- -->

Some properties of k-means
========================================================

- Standard algorithm for k-means clustering is using 
  **Euclidean distance**.
- Different **local optima** (stability) can be reached.

<img src="fig/kmeans_optima.png" style="box-shadow:none;">

***

- **Number of clusters** *K* must be specified beforehand.
    - Elbow method;
    - Silhouette method etc.
    
<img src="fig/elbow.png" style="width:50%;">

<!-- ----------------------------------------------- -->

K-means I: Data
========================================================
incremental: true

```{r km1}
data(Fibulae, package = "archdata")
```
```{r km1-1}
# create matrix
fib <- Fibulae %>% 
  select(
    tot.len = Length, 
    bow.hei = BH,
    foot.len = FL
  ) %>% 
  as.matrix()
head(fib)
```
***
```{r km2}
plot(fib)
```

<!-- ----------------------------------------------- -->

K-means II: Reduce dimensions (PCA)
========================================================
incremental: true
left: 60%

```{r km3}
# scale + PCA
fib_pca <- prcomp(scale(fib))
```
```{r km3-1}
# summary of a PCA
summary(fib_pca)
```

***

```{r km4}
plot(fib_pca$x)
```

<!-- ----------------------------------------------- -->

K-means III: Find optimal K
========================================================

- Package `factoextra`.

```{r km5-0, fig.width=12}
factoextra::fviz_nbclust(fib_pca$x, kmeans)
```

K-means IV
========================================================
incremental: true

```{r km5}
fib_km <- kmeans(fib_pca$x, centers = 2)
fib_km
```

<!-- ----------------------------------------------- -->

K-means V
========================================================
incremental: true
left: 60%

```{r km6}
# cluster assignment
fib_km$cluster
```

```{r km6-1}
# create a data frame with clusters
fib_df <- fib_pca$x %>% 
  as.data.frame() %>% 
  bind_cols(
    clust = fib_km$cluster)

head(fib_df, 4)
```

***

```{r km7}
fib_df %>% 
  ggplot(
    aes(PC1, PC2, 
        color = factor(clust))) +
  geom_point(size = 4) +
  theme_minimal()
```

<!-- ----------------------------------------------- -->

K-means VI
========================================================
title: false

```{r eval=FALSE, echo=FALSE}
centers <- as_tibble(fib_km$centers, rownames = "clust")

fib_df %>%
  ggplot(
    aes(PC1, PC2,
        color = factor(clust))) +
  geom_point(size = 4) +
  theme_minimal() +
  geom_point(data = centers, size = 4, shape = 4) +
  labs(color = "cluster")

ggsave(here::here("lectures/seminar6/fig/kmeans_out.png"))
```

<img src="fig/kmeans_out.png">

<!-- ----------------------------------------------- -->

Hierarchical clustering
========================================================
type: sub-section

<!-- ----------------------------------------------- -->

Hierarchical clustering
========================================================
incremental: true

- The number of clusters is not specified beforehand.
- Output is a **dendrogram** - a hierarchical structure 
  visualizing the cluster growth.
  
- Starts with a distance matrix.

### Agglomerative algorithm:
1. Put each object in its own cluster;
2. Join the clusters that are the closest;
3. Iterate until a single cluster encompassing all objects is reached.

***

#### Agglomerative:
- Builds the hierarchy of clusters from **bottom-up** until
  a single cluster is reached.

#### Divisive:
- Divides a single large cluster into individual objects (**top-down**).
  
  
<img src="fig/hierarchical_clust2.png" style="width:80%;">

<!-- ----------------------------------------------- -->

Hierarchical clustering algorithms
========================================================
title: false

<img src="fig/hierarchical_algorithm.png" style="width:90%; box-sahdow:none;">

<!-- ----------------------------------------------- -->

Hierarchical clustering I: Data
========================================================
left: 40%
incremental: true

```{r hclust1}
# data set same as in k-means
head(fib)
```
```{r hclust2}
# create distance matrix
fib_dist <- dist(fib) 
class(fib_dist)
```

***

```{r hclust3}
as.matrix(fib_dist)[1:6, 1:4]
```

- Matrix with *n* rows and *n* columns.
- Pair-wise distances between the objects.
- Notice the 0 on diagonal, ie. distance to itself is 0.

<!-- ----------------------------------------------- -->

Hierarchical clustering II: Dendrogram
========================================================

```{r hclust4}
fib_hclust <- hclust(fib_dist)
fib_hclust
```

***

```{r hclust5, fig.width=10}
plot(fib_hclust)
```

<!-- ----------------------------------------------- -->

Hierarchical clustering III: Clusters
========================================================
incremental: true

- Different options where to *cut* the dendrogram...

```{r hclust6, echo=FALSE, fig.width=10}
plot(fib_hclust)
abline(h = 45, col = "red", add = TRUE)
abline(h = 80, col = "green", add = TRUE)
abline(h = 35, col = "blue", add = TRUE) 
```

***

- `cutree(x, k, h)` function:
  - specify **k** - number of cluster;
  - specify **h** - height where to cut.

```{r hclust7}
h35 <- cutree(fib_hclust, h = 35)
h35
```

```{r hclust8}
k3 <- cutree(fib_hclust, k = 3)
k3
```

<!-- ----------------------------------------------- -->

Hierarchical clustering IV
========================================================

```{r hclust9, fig.width=10}
plot(fib, col = h35)
```

***

```{r hclust10, fig.width=10}
plot(fib, col = k3)
```

<!-- ----------------------------------------------- -->

Cluster linkage methods I
========================================================

```{r link0, echo=FALSE}
plot_linkage <- function(m) {
  plot(hclust(fib_dist, method = m))
}

```


### Complete (maximum) linkage
- Largest distance between clusters.
- `method = "complete"`

```{r link1, echo=FALSE}
plot_linkage("complete")
```

***

### Single (minimum) linkage
- Smallest distance between clusters.
- `method = "single"`

```{r link2, echo=FALSE}
plot_linkage("single")
```

<!-- ----------------------------------------------- -->

Cluster linkage methods II
========================================================

### Mean (average) linkage
- Mean distance between clusters.
- `method = "average"`

```{r link3, echo=FALSE}
plot_linkage("average")
```

***

### Wards method
- Minimizes total within cluster variance.
- `method = "ward.D2"`

```{r link4, echo=FALSE}
plot_linkage("ward.D2")
```

<!-- ----------------------------------------------- -->

Differences
========================================================
title: false
type: prompt

## K-means partitioning

- Pre-specified number of clusters.
- Clusters may vary (different *local optima*).
- Best when groups in data are *hyper*-spheres.

<img src="fig/kmeans.jpeg">  

***

## Hierarchical clustering

- Variable cluster numbers.
- Clusters are stable.
- Any shape of data distribution.

<img src="fig/hierarchical_clust.png" style="width:60%;">

<!-- ----------------------------------------------- -->

Comparison of k-means vs hclust for k = 2
========================================================
type: prompt

```{r echo=FALSE}
clust_for_k <- function(k) {
  fib %>% 
  as.data.frame() %>% 
  bind_cols(km = kmeans(fib, k)$cluster,
            hc = cutree(fib_hclust, k))
}
```

### K-means clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(2) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(km))) +
  geom_point(size = 6) +
  theme_minimal()
```

***

### Hierarchical clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(2) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(hc))) +
  geom_point(size = 6) +
  theme_minimal()
```

<!-- ----------------------------------------------- -->

Comparison of k-means vs hclust for k = 3
========================================================
type: prompt

### K-means clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(3) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(km))) +
  geom_point(size = 6) +
  theme_minimal()
```

***

### Hierarchical clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(3) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(hc))) +
  geom_point(size = 6) +
  theme_minimal()
```

<!-- ----------------------------------------------- -->

Comparison of k-means vs hclust for k = 4
========================================================
type: prompt

### K-means clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(4) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(km))) +
  geom_point(size = 6) +
  theme_minimal()
```

***

### Hierarchical clustering
```{r echo=FALSE, fig.width=10}
clust_for_k(4) %>% 
  ggplot(aes(tot.len, bow.hei, color = factor(hc))) +
  geom_point(size = 6) +
  theme_minimal()
```

<!-- ----------------------------------------------- -->

