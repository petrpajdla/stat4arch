stat4arch
========================================================
author: Petr Pajdla & Peter Tkáč
autosize: true
width: 1920
height: 1080

AES_707: *Statistics seminar for archaeologists*

<!-- <https://support.rstudio.com/hc/en-us/articles/200486468> -->

<!-- =============================================== -->

Seminar 6
========================================================
type: section

`28. 4. 2022`

Today:
<ul style='color:white;'>
<li>Multivariate statistics
<li>Dimensionality reduction
<li>Clustering
</ul>

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
```


<!-- =============================================== -->

Multivariate data & methods
========================================================
type: section

<!-- ----------------------------------------------- -->

Multivariate data
========================================================

### Division based on number of studied variables

- Univariate  
(one variable per observation);
- Bivariate  
(two variables);
- **Multivariate**  
(also *multidimensional*, multiple variables).

***

<img src="fig/mvn.png" style='box-shadow:none; width:100%;'>

<!-- ----------------------------------------------- -->

Dimensionality reduction
========================================================
type: sub-section

<!-- ----------------------------------------------- -->

The curse of higher dimensions
========================================================
title: false
left: 60%

<img src="fig/curse.png" style="width:100%; box-shadow:none;">

***

The *curse* of higher dimensions...

- Computational ineffectivity;
- Low data density in higher dimensions;
- Problematic visualization, we do not easily cope with more than 3D;
- Difficult interpretation *and much more*.


<!-- ----------------------------------------------- -->

Principal components analysis
========================================================

**Goal:** Find low-dimensional representation of the observations that explain a good fraction of the variation.

- First principal component is a direction that maximizes the 
variance of the projected data.

<img src='fig/pca.png' style="width:100%; box-shadow:none;">

<!-- ----------------------------------------------- -->

PCA I: Data
========================================================
left: 60%

### Data preparation
- Works with numeric data only.
- Format the input as a **matrix**...

```{r pca1}
# load data set
data("DartPoints", package = "archdata")
dp <- DartPoints %>% 
  select(Length, Width) %>% # select columns
  as.matrix() # format as a matrix
# set row names with IDs
rownames(dp) <- DartPoints$Catalog
head(dp)
```

***

### Plot the original data
```{r pca2}
plot(dp)
```

<!-- ----------------------------------------------- -->

PCA II
========================================================
left: 60%

### Do the PCA

```{r pca3}
dp_pca <- prcomp(dp)
dp_pca
summary(dp_pca)
```

***

```{r pca4}
plot(dp_pca)
```

*Scree* plot

- Shows the amount of variation explained by each PC.

<!-- ----------------------------------------------- -->

PCA III: Biplot
========================================================

```{r pca5, fig.width=9, fig.height=9}
biplot(dp_pca)
```

***

```{r pca6, fig.width=9, fig.height=9}
ggbiplot::ggbiplot(dp_pca)
```

<!-- ----------------------------------------------- -->

PCA IV: The details
========================================================

```{r pca7}
# structure of the prcomp object
str(dp_pca)
```

***

```{r pca8}
# extract the coordinates
dp_pca_x <- dp_pca$x
head(dp_pca_x)
class(dp_pca_x)
dp_pca_x <- as.data.frame(dp_pca_x)
class(dp_pca_x)
```

<!-- ----------------------------------------------- -->

PCA V: Custom plot
========================================================
left: 65%

```{r pca9, eval=FALSE, echo=FALSE}
# p1 <- dp_pca_x %>% 
#   bind_cols(Name = DartPoints$Name) %>% 
#   ggplot(aes(PC1, PC2, color = Name)) +
#   geom_point() +
#   coord_fixed() +
#   theme_minimal() +
#   labs(title = "PCA space of DartPoints Length and Width")
# 
# p2 <- DartPoints %>% 
#   ggplot(aes(Length, Width, color = Name)) +
#   geom_point() +
#   coord_fixed() +
#   theme_minimal() +
#   labs(title = "DartPoints scatterplot of Length and Width")
# 
# library(patchwork)
# p <- p1 / p2
# 
# ggsave(here::here("lectures/seminar6/fig/pca_dp.png"), p, 
#        width = 14, height = 8)
```

<img src="fig/pca_dp.png">

***

```{r eval=FALSE}
dp_pca_x %>% 
  bind_cols(
    type = DartPoints$Name
  ) %>% 
  ggplot(
    aes(PC1, PC2, 
        color = type)
  ) +
  geom_point()
```

<!-- =============================================== -->

Clustering
========================================================
type: section

<!-- ----------------------------------------------- -->

Clustering
========================================================
left: 76%

- Broad set of techniques for finding **subgroups**.
- Observations **within groups are quite similar to each other** and observations **in different groups quite different from each other**.

### Clustering methods

- Partitioning: **k-means clustering**
- Agglomerative: **herarchical clustering**
- Model-based: mixture models

***

<img src="fig/kmeans.jpeg">  
<img src="fig/hierarchical_clust.png">

<!-- ----------------------------------------------- -->

K-means partitioning
========================================================
type: sub-section

<!-- ----------------------------------------------- -->

K-means partitioning
========================================================

- Simple approach to partitioning a data set into **K** distinct,
non-overlapping clusters.
- **K** is specified beforehand.

<img src="fig/kmeans_nr_K.png" style="width:76%;">  
<p style="font-size:60%;">(James et al. 2013)</p>

<!-- ----------------------------------------------- -->

K-means algorithm
========================================================
left: 42%
title: false

## K-means algorithm

- *K* ... number of clusters.  

### The process: 

1. **Randomly** assign each observation to groups 1 to *K*;
2. For each of *K* clusters, derive its **centroid** (midpoint);
3. **Reassign** each observation into a cluster whose centroid is 
the **closest**;
4. Iterate until stability in cluster assignments is reached (*local optima*).

***

<img src="fig/kmeans_algorithm.png" style="width:80%;">  
<p style="font-size:60%;">(James et al. 2013)</p>

<!-- ----------------------------------------------- -->

Some properties of k-means
========================================================

- The need to determine the **number of clusters** beforehand.
- Different **local optima** (stability) can be reached.

<img src="fig/kmeans_optima.png" style="box-shadow:none;">

<!-- ----------------------------------------------- -->

K-means I: Data
========================================================

```{r km1}
# load data
data(Fibulae, package = "archdata")
# create matrix
fib <- Fibulae %>% 
  select(
    tot.len = Length, 
    bow.hei = BH
  ) %>% 
  as.matrix()
head(fib)
```
***
```{r km2}
plot(fib)
```

<!-- ----------------------------------------------- -->

K-means II: Reduce dimensions (PCA)
========================================================

```{r km3}
# scale + PCA
fib_pca <- prcomp(scale(fib))
# summary of a PCA
summary(fib_pca)
```

***

```{r km4}
plot(fib_pca$x)
```

<!-- ----------------------------------------------- -->

K-means III
========================================================

```{r km5}
fib_km <- kmeans(fib_pca$x, centers = 3)
fib_km
```

<!-- ----------------------------------------------- -->

K-means IV:
========================================================

```{r km6}
# cluster assignment
fib_km$cluster

# create a data frame with clusters
fib_df <- fib_pca$x %>% 
  as.data.frame() %>% 
  bind_cols(
    clust = fib_km$cluster)

head(fib_df, 4)
```

***

```{r km7}
fib_df %>% 
  ggplot(
    aes(PC1, PC2, 
        color = factor(clust))) +
  geom_point(size = 4) +
  theme_minimal()
```

<!-- ----------------------------------------------- -->

K-means IV:
========================================================
title: false

```{r eval=FALSE, echo=FALSE}
# centers <- as_tibble(fib_km$centers, rownames = "clust")
# 
# fib_df %>% 
#   ggplot(
#     aes(PC1, PC2, 
#         color = factor(clust))) +
#   geom_point(size = 4) +
#   theme_minimal() +
#   geom_point(data = centers, size = 4, shape = 4) +
#   labs(color = "cluster")
# 
# ggsave(here::here("lectures/seminar6/fig/kmeans_out.png"))
```

<img src="fig/kmeans_out.png">

<!-- ----------------------------------------------- -->

Hierarchical clustering
========================================================
type: sub-section

<!-- ----------------------------------------------- -->

<!-- ----------------------------------------------- -->

<!-- ----------------------------------------------- -->