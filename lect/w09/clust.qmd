---
title: "Clustering"
format:
  html: default
  revealjs:
    fontsize: 24px
    output-file: slides_clust.html
    footer: "[AES_707](https://petrpajdla.github.io/stat4arch/) *Statistics seminar for archaeologists* | [Clustering](./clust.html)"
    chalkboard:
      theme: whiteboard
      chalk-effect: 0.0
  # pdf: default
execute:
  echo: true
  warning: false
  cache: true
---


## Clustering

:::: {.columns}
::: {.column width=60%}
- Broad set of techniques for finding **subgroups**.
- Observations **within groups are quite similar to each other** and observations in **different groups quite different from each other**.

### Clustering methods

- Partitioning: **k-means** clustering
- Agglomerative/divisive: **hierarchical** clustering
- Model-based: mixture models
:::
::: {.column width=40%}
![](figs/kmeans.jpeg){width=60%}

![](figs/hierarchical_clust.png){width=60%}
:::
::::




## K-means clustering

- Simple approach to partitioning a data set into **K distinct, non-overlapping clusters**.
- **K is specified beforehand**.

![James et al. 2013](figs/kmeans_nr_K.png)


## K-means algorithm

:::: {.columns}
::: {.column width=40%}
- $K$ -- number of clusters.

::: {.fragment}
### The process:

:::{.incremental}
1. **Randomly** assign each observation to groups **1 to K**;
1. For each of K clusters, derive its **centroid** (midpoint);
1. **Reassign** each observation into a cluster whose centroid is the **closest**;
1. **Iterate** until stability in cluster assignments is reached (local vs global optima).
:::
:::
:::
::: {.column width=60%}
![James et al. 2013](figs/kmeans_algorithm.png)
:::
::::


## Some properties of K-means partitioning

:::: {.columns}
::: {.column width=50%}
- Standard algorithm for k-means clustering is using **Euclidean distance** (distances are calculated using *Pythagorean theorem*).
- Different **local optima** (stability) can be reached.

![](figs/kmeans_optima.png)
:::
::: {.column width=50% .fragment}
- **Number of clusters *K*** must be specified in advance 
- How to determine **optimal** number of clusters?
    
    - Elbow method,
    -  Silhouette method

![](figs/elbow.png){width=60%}
:::
::::




## Hierarchical clustering 

:::: {.columns}
::: {.column width=50%}
- The number of clusters is **not specified beforehand**.
- Output is a **dendrogram** -- a hierarchical structure visualizing the cluster growth.
- Starts with a **distance matrix**.
:::
::: {.column width=50%}
![](figs/hierarchical_clust2.png){width=80%}
:::
::::

:::: {.columns .fragment}
::: {.column width=50%}
### Agglomerative

- Builds the hierarchy of clusters from **bottom-up** until a **single cluster is reached**.

1. Put each object in its own cluster;
1. Join the clusters that are the closest;
1. Iterate until a single cluster encompassing all objects is reached.
:::
::: {.column width=50%}
### Divisive

- **Divides** a single large cluster into **individual objects** (top-down).

1. Put all objects into a single cluster;
2. Divide the cluster into subclusters at a similar distance;
3. Iterate util all objects are in their own clusters.
:::
::::

## Hierarchical clustering algorithms

![](figs/hierarchical_algorithm.png)



## Clustering methods comparison

:::: {.columns}
::: {.column width=50%}
### K-means partitioning 

- Pre-specified number of clusters.
- Clusters may vary (different local optima).
- Best when groups in data are (hyper)spheres.

![](figs/kmeans.jpeg){width=80%}
:::
::: {.column width=50%}
### Hierarchical clustering 

- Variable cluster numbers.
- Clusters are stable.
- Any shape of data distribution.

![](figs/hierarchical_clust.png){width=80%}
:::
::::
